{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-1 EW RTC with pyroSAR (SNAP)\n",
    "\n",
    "**Overview**\n",
    "- Notebook for creating Radiometrically Terrain Corrected (RTC) data for Sentinel-1 EW using the pyroSAR software\n",
    "- This notebook will search and download the following data:\n",
    "    - Sentinel-1 SLC \n",
    "    - Precise Orbit Files\n",
    "    - Copernicus 30m DEM covering region of Interest\n",
    "- RTC backscatter will be created with the pyroSAR software package (SNAP backend)\n",
    "\n",
    "**Requirements**\n",
    "- A conda environment setup as described in the *README.md*\n",
    "- Appropriate storage space\n",
    "- Credentials for usgs (https://urs.earthdata.nasa.gov/users/new) \n",
    "- Credentials for copernicus CDSE ( https://dataspace.copernicus.eu/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "import os\n",
    "import asf_search as asf\n",
    "from eof.download import download_eofs\n",
    "import yaml\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import shutil\n",
    "from shapely import Polygon, box\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import rasterio\n",
    "import rasterio.merge\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from pyroSAR.snap import geocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the path the the snap binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path the the snap binary\n",
    "# Default location on NCI is $HOME/snap/bin\n",
    "# Can also export from command line :\n",
    "# export PATH=\"$PATH:$HOME/snap/bin\"\n",
    "SNAP_PATH = '/home/547/ab7271/snap/bin'\n",
    "if 'snap' not in os.environ['PATH']:\n",
    "    os.environ['PATH'] = os.environ['PATH'] + ':' + SNAP_PATH\n",
    "print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_dir =  '/g/data/yp75/projects/sar-antractica-processing/alexf_ew/data' #'/home/547/ab7271/s1-rtc-isce3-notebook/data' # set this as the absolute path\n",
    "download_folder = os.path.join(data_dir,'scenes')\n",
    "earthdata_credentials_path = \"credentials/credentials_earthdata.yaml\"\n",
    "copernicus_credentials_path = \"credentials/credentials_copernicus.yaml\"\n",
    "precise_orb_download_folder = os.path.join(data_dir,'orbits','POEORB')\n",
    "restituted_orb_download_folder = os.path.join(data_dir,'orbits','RESORB')\n",
    "rtc_outpath = os.path.join(data_dir,'processed_scene_snap')\n",
    "rtc_scratch_dir = os.path.join(data_dir,'temp')\n",
    "dem_download_folder = os.path.join(data_dir,'dem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folders = True\n",
    "if create_folders:\n",
    "    for f in [\n",
    "        download_folder, \n",
    "        precise_orb_download_folder, \n",
    "        restituted_orb_download_folder,\n",
    "        dem_download_folder,\n",
    "        rtc_outpath,\n",
    "        rtc_scratch_dir\n",
    "        ]:\n",
    "        os.makedirs(f, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(earthdata_credentials_path, \"r\", encoding='utf8') as f:\n",
    "        earthdata_credentials = yaml.safe_load(f.read())\n",
    "with open(copernicus_credentials_path, \"r\", encoding='utf8') as f:\n",
    "        copernicus_credentials = yaml.safe_load(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search and Download Scene of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1 - Search based on a geometry and daterange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wkt = \"POINT (130.7085216 -16.0044816)\" # South Australia\n",
    "wkt = \"POINT (110.526792 -66.282343)\" # Casey station Antarctica\n",
    "print(f'Searching over point: {wkt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product type https://github.com/asfadmin/Discovery-asf_search/blob/master/asf_search/constants/PRODUCT_TYPE.py\n",
    "# prod = 'GRD_HS' # IW\n",
    "prod = 'GRD_MD' # EW\n",
    "\n",
    "# prod = 'GRD_HD'\n",
    "# prod = 'GRD_MS'\n",
    "# prod = 'GRD_FD'\n",
    "# prod = 'SLC'\n",
    "\n",
    "# acquisition mode\n",
    "#mode = 'IW'\n",
    "mode = 'EW'\n",
    "\n",
    "results = asf.search(platform=[asf.PLATFORM.SENTINEL1], \n",
    "                     intersectsWith=wkt, \n",
    "                     maxResults=1, \n",
    "                     processingLevel=prod,\n",
    "                     beamMode=mode,\n",
    "                     start='2021-12-31T11:59:59Z',\n",
    "                     end='2022-01-19T11:59:59Z')\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2 - Search for known products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_list = True\n",
    "if search_list:\n",
    "\n",
    "    # prod = 'SLC'\n",
    "    # mode = 'IW'\n",
    "    prod = 'GRD_MD'\n",
    "    mode = 'EW'\n",
    "    \n",
    "    scene_list = [\n",
    "        #'S1A_EW_GRDM_1SDH_20210701T150428_20210701T150533_038585_048D91_23E6', \n",
    "        'S1A_EW_GRDM_1SDH_20210704T152833_20210704T152942_038629_048EE8_D41F'\n",
    "   ]\n",
    "\n",
    "    # search the scene list with the specified product type\n",
    "    results = asf.granule_search(scene_list, asf.ASFSearchOptions(processingLevel=prod, beamMode=mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in enumerate(results):\n",
    "    print(f\"{i+1} - {r.properties['sceneName']}\")\n",
    "    print(r.properties)\n",
    "    print(r.geometry)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download\n",
    "session = asf.ASFSession()\n",
    "session.auth_with_creds(earthdata_credentials['login'],earthdata_credentials['password'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit to the first scene\n",
    "results = results[0:1]\n",
    "geometry = results[0].geometry\n",
    "scene = results[0].properties['sceneName']\n",
    "pol = results[0].properties['polarization']\n",
    "name = str(results[0].properties['sceneName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Downloading {len(results)} scenes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all results\n",
    "scene_paths = []\n",
    "scene_names = []\n",
    "for s in results:\n",
    "    name = s.properties['sceneName']\n",
    "    scene_names.append(name)\n",
    "    print(name)\n",
    "    path = os.path.join(download_folder, name)\n",
    "    s.download(path=download_folder, session=session)\n",
    "    scene_paths.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Precise Orbit Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_orbits = True\n",
    "# download not required, happens in pyrosar geocode process\n",
    "if download_orbits:\n",
    "    for scene in  scene_paths:\n",
    "        # download precise orbits\n",
    "        download_eofs(\n",
    "            sentinel_file=scene, \n",
    "            save_dir=precise_orb_download_folder, \n",
    "            orbit_type='precise', \n",
    "            cdse_user=copernicus_credentials['login'], \n",
    "            cdse_password=copernicus_credentials['password'],\n",
    "            asf_user=earthdata_credentials['login'], \n",
    "            asf_password=earthdata_credentials['password']\n",
    "            )\n",
    "        # download restituted orbits\n",
    "        download_eofs(\n",
    "            sentinel_file=scene, \n",
    "            save_dir=restituted_orb_download_folder, \n",
    "            orbit_type='restituted', \n",
    "            cdse_user=copernicus_credentials['login'], \n",
    "            cdse_password=copernicus_credentials['password'],\n",
    "            asf_user=earthdata_credentials['login'], \n",
    "            asf_password=earthdata_credentials['password']\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('precise orbits')\n",
    "for p in os.listdir(precise_orb_download_folder):\n",
    "    prec_orb_path = os.path.join(precise_orb_download_folder,p)\n",
    "    print(prec_orb_path)\n",
    "\n",
    "print('restited orbits')\n",
    "for p in os.listdir(restituted_orb_download_folder):\n",
    "    res_orb_path = os.path.join(restituted_orb_download_folder,p)\n",
    "    print(res_orb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process DEMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_required_dem_tile_paths_by_filename(\n",
    "    bounds: tuple,\n",
    "    check_exists: bool = True,\n",
    "    cop30_folder_path = '/g/data/v10/eoancillarydata-2/elevation/copernicus_30m_world/',\n",
    "    search_buffer=0.3,\n",
    "    tifs_in_subfolder=True,\n",
    ") -> list[str]:\n",
    "    \"\"\"generate a list of the required dem paths based on the bounding coords. The\n",
    "    function searches the specified folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bounds : tuple\n",
    "        the set of bounds (min_lon, min_lat, max_lon, max_lat)\n",
    "    check_exists : bool, optional\n",
    "        Check if the file exists, by default True\n",
    "    cop30_folder_path : str, optional\n",
    "        path to the tile folders, by default COP30_FOLDER_PATH\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        list of paths for required dem tiles in bounds\n",
    "    \"\"\"\n",
    "\n",
    "    # add a buffer to the search\n",
    "    bounds = box(*bounds).buffer(search_buffer).bounds\n",
    "    # logic to find the correct files based on data being stored in each tile folder\n",
    "    min_lat = np.floor(bounds[1]) if bounds[1] < 0 else np.ceil(bounds[1])\n",
    "    max_lat = np.ceil(bounds[3]) if bounds[3] < 0 else np.floor(bounds[3]) + 1\n",
    "    min_lon = np.floor(bounds[0]) if bounds[0] < 0 else np.floor(bounds[0])\n",
    "    max_lon = np.ceil(bounds[2]) if bounds[2] < 0 else np.ceil(bounds[2])\n",
    "    lat_range = list(range(int(min_lat), int(max_lat)))\n",
    "    lon_range = list(range(int(min_lon), int(max_lon)))\n",
    "    print(f\"lat tile range: {lat_range}\")\n",
    "    print(f\"lon tile range: {lon_range}\")\n",
    "    dem_paths = []\n",
    "    dem_folders = []\n",
    "\n",
    "    for lat in lat_range:\n",
    "        for lon in lon_range:\n",
    "            lat_dir = \"N\" if lat >= 0 else \"S\"\n",
    "            lon_dir = \"E\" if lon >= 0 else \"W\"\n",
    "            dem_foldername = f\"Copernicus_DSM_COG_10_{lat_dir}{int(abs(lat)):02d}_00_{lon_dir}{int(abs(lon)):03d}_00_DEM\"\n",
    "            if tifs_in_subfolder:\n",
    "                dem_subpath = f\"{dem_foldername}/{dem_foldername}.tif\"\n",
    "            else:\n",
    "                dem_subpath = f\"{dem_foldername}.tif\"\n",
    "            dem_path = os.path.join(cop30_folder_path, dem_subpath)\n",
    "            if check_exists:\n",
    "                # check the file exists, e.g. over water will not be a file\n",
    "                if os.path.exists(dem_path):\n",
    "                    dem_paths.append(dem_path)\n",
    "                    dem_folders.append(dem_foldername)\n",
    "            else:\n",
    "                dem_paths.append(dem_path)\n",
    "    return sorted(list(set(dem_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer the sceme nounda to ensure coverage\n",
    "scene_bounds = Polygon(geometry['coordinates'][0]).bounds\n",
    "print(f'original scene bounds : {scene_bounds}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust DEM bounds to ensure area is covered\n",
    "- Note at high latitudes this may not be sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_degrees = 1\n",
    "scene_poly = box(*scene_bounds)\n",
    "expanded_scene_boounds = scene_poly.buffer(buffer_degrees).bounds\n",
    "# Create the polygon\n",
    "geom = Polygon([(expanded_scene_boounds[0], expanded_scene_boounds[1]), \n",
    "                (expanded_scene_boounds[2], expanded_scene_boounds[1]), \n",
    "                (expanded_scene_boounds[2], expanded_scene_boounds[3]), \n",
    "                (expanded_scene_boounds[0], expanded_scene_boounds[3])])\n",
    "expanded_scene_geom = list(geom.exterior.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'original bounds : {scene_bounds}')\n",
    "print(f'expanded bounds : {expanded_scene_boounds}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lat, max_lat  = min([c[1] for c in expanded_scene_geom]), max([c[1] for c in expanded_scene_geom])\n",
    "min_lon, max_lon  = min([c[0] for c in expanded_scene_geom]), max([c[0] for c in expanded_scene_geom])\n",
    "lats = list(range(int(np.floor(min_lat)), int(np.ceil(max_lat+1))))\n",
    "longs = list(range(int(np.floor(min_lon)), int(np.ceil(max_lon+1))))\n",
    "print(expanded_scene_geom)\n",
    "print(lats)\n",
    "print(longs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1 - Find and download Copernicus 30m DEM from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_s3_paths = []\n",
    "\n",
    "for lat in lats:\n",
    "    for long in longs:\n",
    "        if lat >= 0:\n",
    "            lat_dir = \"N\"\n",
    "        else:\n",
    "            lat_dir = \"S\"\n",
    "        if long >= 0:\n",
    "            long_dir = \"E\"\n",
    "        else:\n",
    "            long_dir = \"W\"\n",
    "        \n",
    "        dem_s3_paths.append(f\"Copernicus_DSM_COG_10_{lat_dir}{int(abs(lat)):02d}_00_{long_dir}{int(abs(long)):03d}_00_DEM/Copernicus_DSM_COG_10_{lat_dir}{int(abs(lat)):02d}_00_{long_dir}{int(abs(long)):03d}_00_DEM.tif\")\n",
    "dem_s3_paths = list(set(dem_s3_paths))\n",
    "print(f'DEM tiles to download : {len(dem_s3_paths)}')\n",
    "#dem_s3_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3', config=Config(signature_version=UNSIGNED))\n",
    "bucket_name = \"copernicus-dem-30m\"\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "dems_to_merge = []\n",
    "\n",
    "for s3_path in dem_s3_paths:\n",
    "    try:\n",
    "        dl_path = os.path.join(dem_download_folder,s3_path.split('/')[1])\n",
    "        if not os.path.exists(dl_path):\n",
    "            bucket.download_file(s3_path, dl_path)\n",
    "            print(f'downloaded : {s3_path}')\n",
    "            dems_to_merge.append(dl_path)\n",
    "        else:\n",
    "            print(f'file exists : {s3_path}')\n",
    "            dems_to_merge.append(dl_path)\n",
    "    except:\n",
    "        print(f'not found : {s3_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_paths = ' '.join(dems_to_merge)\n",
    "dem_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2 - Use Local Cop30m Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP30_FOLDER_PATH = \"/g/data/v10/eoancillarydata-2/elevation/copernicus_30m_world/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expanded_scene_boounds)\n",
    "dem_paths = find_required_dem_tile_paths_by_filename(expanded_scene_boounds, search_buffer=0,cop30_folder_path=COP30_FOLDER_PATH)\n",
    "print(f'Number of DEMS to merge {len(dem_paths)}')\n",
    "dem_paths = ' '.join(dem_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dem_path = os.path.join(dem_download_folder,f\"{name}_dem.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the proj location if it raises a warning\n",
    "os.environ['PROJ_LIB'] = '/g/data/yp75/ab7271/microconda/envs/pyrosar_rtc/share/proj'\n",
    "! echo $PROJ_LIB\n",
    "!gdal_merge.py -n 0 -o $merged_dem_path $dem_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_no_data_value(input_file, output_file, no_data_value=-9999):\n",
    "    with rasterio.open(input_file, \"r+\") as src:\n",
    "        nodata = src.nodata\n",
    "        if nodata in [None, np.nan, 'nan','np.nan','np.Nan']:\n",
    "            print(f'replace dem nodata from np.nan to -9999')\n",
    "            src.nodata = no_data_value\n",
    "            with rasterio.open(output_file, 'w',  **src.profile) as dst:\n",
    "                dem_data = dst.read(1)\n",
    "                dem_data[dem_data==np.nan] = -9999\n",
    "                dem_data[dem_data=='nan'] = -9999\n",
    "                dem_data[dem_data==None] = -9999\n",
    "                dst.write(dem_data, 1)\n",
    "                dst.update_tags(AREA_OR_POINT='Point')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyroSAR cant handle a nodata value of np.nan\n",
    "# we therefore set this to be -9999\n",
    "fix_no_data_value(merged_dem_path, merged_dem_path, no_data_value=-9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce PyroSAR RTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "spacing = 40\n",
    "scaling = 'linear' # scale of final product, linear, db\n",
    "refarea = 'gamma0' # e.g. gamma0, sigma0, beta0 or ['gamma0','sigma0']\n",
    "t_crs = 3031\n",
    "export_extra = [\"localIncidenceAngle\",\"DEM\",\"layoverShadowMask\",\"scatteringArea\",\"gammaSigmaRatio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide args to snap/java to limit memory etc (provide as list)\n",
    "# https://forum.step.esa.int/t/gpt-and-snap-performance-parameters-exhaustive-manual-needed/8797 \n",
    "# https://github.com/johntruckenbrodt/pyroSAR/blob/main/pyroSAR/snap/util.py#L158\n",
    "gpt_args = ['-x']\n",
    "# gpt_args = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, you may need to clear the results folder if the scene has been run before\n",
    "# note function downloads orbits, might cause issue with batch NCI run\n",
    "scene_workflow = geocode(\n",
    "    infile=os.path.join(download_folder,f\"{scene}.zip\"),\n",
    "    outdir=rtc_outpath,\n",
    "    allow_RES_OSV=True,\n",
    "    externalDEMFile=merged_dem_path,\n",
    "    externalDEMNoDataValue=-9999,\n",
    "    externalDEMApplyEGM=True, \n",
    "    spacing=spacing,\n",
    "    scaling=scaling,\n",
    "    refarea=refarea,\n",
    "    t_srs=t_crs,\n",
    "    returnWF=True,\n",
    "    clean_edges=True,\n",
    "    export_extra=export_extra,\n",
    "    gpt_args=gpt_args,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change some parameters of the config to point to our files\n",
    "\n",
    "- The run is based off of this .yaml file, so there may be additonal changes you want to make\n",
    "\n",
    "- For example, additional metadata, projections and output resolutions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrosar_rtc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
