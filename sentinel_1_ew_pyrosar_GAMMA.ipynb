{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-1 EW RTC with pyroSAR (GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "import os\n",
    "import asf_search as asf\n",
    "from eof.download import download_eofs\n",
    "import yaml\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import shutil\n",
    "from shapely import Polygon, box\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import rasterio\n",
    "import rasterio.merge\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from pyroSAR.snap import geocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA_HOME_PATH = \"/g/data/dg9/GAMMA/GAMMA_SOFTWARE-20230712\"\n",
    "\n",
    "if os.environ.get(\"GAMMA_HOME\", None) is not None:\n",
    "    os.environ[\"GAMMA_HOME\"] = GAMMA_HOME_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm PyroSAR can access GAMMA install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyroSAR.examine import ExamineGamma\n",
    "config = ExamineGamma()\n",
    "\n",
    "print(config.home)\n",
    "print(config.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_dir =  '/g/data/yp75/ca6983/data'\n",
    "download_folder = os.path.join(data_dir,'scenes')\n",
    "earthdata_credentials_path = \"credentials/credentials_earthdata.yaml\"\n",
    "copernicus_credentials_path = \"credentials/credentials_copernicus.yaml\"\n",
    "precise_orb_download_folder = os.path.join(data_dir,'orbits','POEORB')\n",
    "restituted_orb_download_folder = os.path.join(data_dir,'orbits','RESORB')\n",
    "rtc_outpath = os.path.join(data_dir,'results')\n",
    "rtc_scratch_dir = os.path.join(data_dir,'scratch')\n",
    "dem_download_folder = os.path.join(data_dir,'dem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folders = True\n",
    "if create_folders:\n",
    "    for f in [\n",
    "        download_folder, \n",
    "        precise_orb_download_folder, \n",
    "        restituted_orb_download_folder,\n",
    "        dem_download_folder,\n",
    "        rtc_outpath,\n",
    "        rtc_scratch_dir\n",
    "        ]:\n",
    "        os.makedirs(f, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(earthdata_credentials_path, \"r\", encoding='utf8') as f:\n",
    "        earthdata_credentials = yaml.safe_load(f.read())\n",
    "with open(copernicus_credentials_path, \"r\", encoding='utf8') as f:\n",
    "        copernicus_credentials = yaml.safe_load(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1_zip_file = \"/g/data/fj7/Copernicus/Sentinel-1/C-SAR/GRD/2024/2024-01/70S170E-75S175E/S1A_EW_GRDM_1SDH_20240129T091735_20240129T091828_052319_065379_0F1E.zip\"\n",
    "\n",
    "scene_id = \"S1A_EW_GRDM_1SDH_20240129T091735_20240129T091828_052319_065379_0F1E\"\n",
    "prod = \"GRD_MD\"\n",
    "mode = \"EW\"\n",
    "\n",
    "results = asf.granule_search([scene_id], asf.ASFSearchOptions(processingLevel=prod, beamMode=mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = asf.ASFSession()\n",
    "session.auth_with_creds(earthdata_credentials['login'],earthdata_credentials['password'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit to the first scene\n",
    "results = results[0:1]\n",
    "geometry = results[0].geometry\n",
    "scene = results[0].properties['sceneName']\n",
    "pol = results[0].properties['polarization']\n",
    "name = str(results[0].properties['sceneName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all results\n",
    "scene_paths = []\n",
    "scene_names = []\n",
    "for s in results:\n",
    "    name = s.properties['sceneName']\n",
    "    scene_names.append(name)\n",
    "    print(name)\n",
    "    path = os.path.join(download_folder, name)\n",
    "    s.download(path=download_folder, session=session)\n",
    "    scene_paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyroSAR import identify\n",
    "\n",
    "id = identify(scene_paths[0] + \".zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_polygon(src_crs, dst_crs, geometry, always_xy=True):\n",
    "    src_crs = pyproj.CRS(f\"EPSG:{src_crs}\")\n",
    "    dst_crs = pyproj.CRS(f\"EPSG:{dst_crs}\") \n",
    "    transformer = pyproj.Transformer.from_crs(src_crs, dst_crs, always_xy=always_xy)\n",
    "     # Transform the polygon's coordinates\n",
    "    transformed_exterior = [\n",
    "        transformer.transform(x, y) for x, y in geometry.exterior.coords\n",
    "    ]\n",
    "    # Create a new Shapely polygon with the transformed coordinates\n",
    "    transformed_polygon = Polygon(transformed_exterior)\n",
    "    return transformed_polygon\n",
    "\n",
    "def adjust_scene_poly_at_extreme_lat(bbox, src_crs, ref_crs, delta=0.1):\n",
    "    \"\"\"\n",
    "    Adjust the bounding box around a scene in src_crs (4326) due to warping at high\n",
    "    Latitudes. For example, the min and max bounding values for an antarctic scene in\n",
    "    4326 may not actually be the true min and max due to distortions at high latitudes. \n",
    "\n",
    "    Parameters:\n",
    "    - bbox: Tuple of four coordinates (x_min, y_min, x_max, y_max).\n",
    "    - src_crs: Source EPSG. e.g. 4326\n",
    "    - ref_crs: reference crs to create the true bbox. i.e. 3031 in southern \n",
    "                hemisphere and 3995 in northern (polar stereographic)\n",
    "    - delta: distance between generation points along the bounding box sides in\n",
    "            src_crs. e.g. 0.1 degrees in lat/lon \n",
    "\n",
    "    Returns:\n",
    "    - A polygon bounding box expanded to the true min max\n",
    "    \"\"\"\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    # Generate points along the top side\n",
    "    top_side = [(x, y_max) for x in list(np.arange(x_min, x_max, delta)) + [x_max]]    \n",
    "    # Generate points along the right side\n",
    "    right_side = [(x_max, y) for y in list(np.arange(y_max - delta, y_min-delta, -delta)) + [y_min]]\n",
    "    # Generate points along the bottom side\n",
    "    bottom_side = [(x, y_min) for x in list(np.arange(x_max - delta, x_min-delta, -delta)) + [x_min]]\n",
    "    list(np.arange(y_min + delta, y_max, delta)) + [y_max]\n",
    "    # Generate points along the left side\n",
    "    left_side = [(x_min, y) for y in list(np.arange(y_min + delta, y_max, delta)) + [y_max]]\n",
    "    # Combine all sides' points\n",
    "    all_points = top_side + right_side + bottom_side + left_side\n",
    "    # convert to a polygon \n",
    "    polygon = Polygon(all_points)\n",
    "    # convert polygon to desired crs and get bounds in those coordinates\n",
    "    trans_bounds = transform_polygon(src_crs, ref_crs, polygon).bounds\n",
    "    trans_poly = Polygon(\n",
    "        [(trans_bounds[0], trans_bounds[1]), \n",
    "         (trans_bounds[2], trans_bounds[1]), \n",
    "         (trans_bounds[2], trans_bounds[3]), \n",
    "         (trans_bounds[0], trans_bounds[3])]\n",
    "        )\n",
    "    corrected_poly = transform_polygon(ref_crs, src_crs, trans_poly)\n",
    "    return corrected_poly\n",
    "\n",
    "def expand_raster_with_bounds(input_raster, output_raster, old_bounds, new_bounds, fill_value=None):\n",
    "    \"\"\"Expand the raster to the desired bounds. Resolution and Location are preserved.\n",
    "\n",
    "    Args:\n",
    "        input_raster (str): input raster path\n",
    "        output_raster (str): out raster path\n",
    "        old_bounds (tuple): current bounds\n",
    "        new_bounds (tuple): new bounds\n",
    "        fill_value (float, int, optional): Fill value to pad with. Defaults to None and nodata is used.\n",
    "    \"\"\"\n",
    "    # Open the raster dataset\n",
    "    with rasterio.open(input_raster, 'r') as src:\n",
    "        # get old bounds\n",
    "        old_left, old_bottom, old_right, old_top = old_bounds\n",
    "        # Define the new bounds\n",
    "        new_left, new_bottom, new_right, new_top = new_bounds\n",
    "        # adjust the new bounds with even pixel multiples of existing\n",
    "        # this will stop small offsets\n",
    "        print(f'Making new raster with target bounds: {new_bounds}')\n",
    "        new_left = old_left - int(abs(new_left-old_left)/src.res[0])*src.res[0]\n",
    "        new_right = old_right + int(abs(new_right-old_right)/src.res[0])*src.res[0]\n",
    "        new_bottom = old_bottom - int(abs(new_bottom-old_bottom)/src.res[1])*src.res[1]\n",
    "        new_top = old_top + int(abs(new_top-old_top)/src.res[1])*src.res[1]\n",
    "        print(f'New raster bounds: {(new_left, new_bottom, new_right, new_top)}')\n",
    "        # Calculate the new width and height, should be integer values\n",
    "        new_width = int((new_right - new_left) / src.res[0])\n",
    "        new_height = int((new_top - new_bottom) / src.res[1])\n",
    "        # Define the new transformation matrix\n",
    "        transform = from_origin(new_left, new_top, src.res[0], src.res[1])\n",
    "        # Create a new raster dataset with expanded bounds\n",
    "        profile = src.profile\n",
    "        profile.update({\n",
    "            'width': new_width,\n",
    "            'height': new_height,\n",
    "            'transform': transform\n",
    "        })\n",
    "        # make a temp file\n",
    "        tmp = output_raster.replace('.tif','_tmp.tif')\n",
    "        print(f'Making temp file: {tmp}')\n",
    "        with rasterio.open(tmp, 'w', **profile) as dst:\n",
    "            # Read the data from the source and write it to the destination\n",
    "            fill_value = profile['nodata'] if fill_value is None else fill_value\n",
    "            print(f'Padding new raster extent with value: {fill_value}')\n",
    "            data = np.full((new_height, new_width), fill_value=fill_value, dtype=profile['dtype'])\n",
    "            dst.write(data, 1)\n",
    "        # merge the old raster into the new raster with expanded bounds \n",
    "        print(f'Merging original raster and expanding bounds...')\n",
    "    del data\n",
    "    rasterio.merge.merge(\n",
    "        datasets=[tmp, input_raster],\n",
    "        method='max',\n",
    "        dst_path=output_raster)\n",
    "    os.remove(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer the sceme nounda to ensure coverage\n",
    "scene_bounds = Polygon(geometry['coordinates'][0]).bounds\n",
    "print(f'original scene bounds : {scene_bounds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we are at high latitudes we need to correct the bounds due to the skewed box shape\n",
    "if (scene_bounds[1] < -50) or (scene_bounds[3] < -50):\n",
    "    # Southern Hemisphere\n",
    "    print(f'Adjusting scene bounds due to warping at high latitude')\n",
    "    scene_poly = adjust_scene_poly_at_extreme_lat(scene_bounds, 4326, 3031)\n",
    "    scene_bounds = scene_poly.bounds \n",
    "    print(f'Adjusted scene bounds : {scene_bounds}')\n",
    "if (scene_bounds[1] > 50) or (scene_bounds[3] > 50):\n",
    "    # Northern Hemisphere\n",
    "    print(f'Adjusting scene bounds due to warping at high latitude')\n",
    "    scene_poly = adjust_scene_poly_at_extreme_lat(scene_bounds, 4326, 3995)\n",
    "    scene_bounds = scene_poly.bounds \n",
    "    print(f'Adjusted scene bounds : {scene_bounds}')\n",
    "print('adjusted scene bounds')\n",
    "scene_bounds\n",
    "# Create the polygon\n",
    "geom = Polygon([(scene_bounds[0], scene_bounds[1]), \n",
    "                (scene_bounds[2], scene_bounds[1]), \n",
    "                (scene_bounds[2], scene_bounds[3]), \n",
    "                (scene_bounds[0], scene_bounds[3])])\n",
    "geom = list(geom.exterior.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lat, max_lat  = min([c[1] for c in geom]), max([c[1] for c in geom])\n",
    "min_lon, max_lon  = min([c[0] for c in geom]), max([c[0] for c in geom])\n",
    "lats = list(range(int(np.floor(min_lat)), int(np.ceil(max_lat+1))))\n",
    "longs = list(range(int(np.floor(min_lon)), int(np.ceil(max_lon+1))))\n",
    "print(geom)\n",
    "print(lats)\n",
    "print(longs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_s3_paths = []\n",
    "\n",
    "for lat in lats:\n",
    "    for long in longs:\n",
    "        if lat >= 0:\n",
    "            lat_dir = \"N\"\n",
    "        else:\n",
    "            lat_dir = \"S\"\n",
    "        if long >= 0:\n",
    "            long_dir = \"E\"\n",
    "        else:\n",
    "            long_dir = \"W\"\n",
    "        \n",
    "        dem_s3_paths.append(f\"Copernicus_DSM_COG_10_{lat_dir}{int(abs(lat)):02d}_00_{long_dir}{int(abs(long)):03d}_00_DEM/Copernicus_DSM_COG_10_{lat_dir}{int(abs(lat)):02d}_00_{long_dir}{int(abs(long)):03d}_00_DEM.tif\")\n",
    "dem_s3_paths = list(set(dem_s3_paths))\n",
    "print(f'DEM tiles to download : {len(dem_s3_paths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3', config=Config(signature_version=UNSIGNED))\n",
    "bucket_name = \"copernicus-dem-30m\"\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "dems_to_merge = []\n",
    "\n",
    "for s3_path in dem_s3_paths:\n",
    "    try:\n",
    "        dl_path = os.path.join(dem_download_folder,s3_path.split('/')[1])\n",
    "        if not os.path.exists(dl_path):\n",
    "            bucket.download_file(s3_path, dl_path)\n",
    "            print(f'downloaded : {s3_path}')\n",
    "            dems_to_merge.append(dl_path)\n",
    "        else:\n",
    "            print(f'file exists : {s3_path}')\n",
    "            dems_to_merge.append(dl_path)\n",
    "    except:\n",
    "        print(f'not found : {s3_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_paths = ' '.join(dems_to_merge)\n",
    "dem_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dem_path = os.path.join(dem_download_folder,f\"{name}_dem.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"gdal_merge -n 0 -o {merged_dem_path} {dem_paths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(merged_dem_path) as src:\n",
    "    dem_meta = src.meta.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
